{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Wrangle, prepare, cleanse the data**\n",
    "\n",
    "In this section, we will prepare the data to ensure that the model operates optimally. We will follow a set of steps based on the analysis we conducted in Section 2 Data Analysis. The phases we will work on include:\n",
    "\n",
    "**3.1 Cleaning:** Removing duplicates, correcting incorrect column names, removing irrelevant columns and changing data types.\\\n",
    "**3.2 Integration:** Data grouping and handling missing values.\\\n",
    "**3.3 Construction:** Creating new variables and encoding categorical variables.\\\n",
    "**3.4 Variable selection:** Studying correlations and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,OrdinalEncoder,StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "\n",
    "# Best practices\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_columns',None)  \n",
    "pd.options.display.float_format = '{:.2f}'.format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquire data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_easymoney.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Removing duplicates**\n",
    "\n",
    "We note that we do not have any duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Correcting incorrect column names**\n",
    "\n",
    "We can see how the em_acount variable is misspelled, it should be em_account. Not correcting this error, no matter how insignificant it may seem now, can lead to problems later when working with this column. We proceed to correct the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'pk_cid', 'pk_partition', 'short_term_deposit', 'loans',\n",
       "       'mortgage', 'funds', 'securities', 'long_term_deposit', 'em_account_pp',\n",
       "       'credit_card', 'payroll', 'pension_plan', 'payroll_account',\n",
       "       'emc_account', 'debit_card', 'em_account_p', 'em_acount', 'entry_date',\n",
       "       'entry_channel', 'active_customer', 'segment', 'country_id',\n",
       "       'region_code', 'gender', 'age', 'deceased', 'salary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {\"em_acount\":\"em_account\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Removing inrrelevant columns**\n",
    "\n",
    "Although later we will do correlation and variance studies, where we will delve into those columns that may be irrelevant, taking a quick look we can see that:\n",
    "\n",
    "* 'Unnamed:0' : This column acts as an inndex of the rows of our sample. This column is obviusly irrelevant since the dataframe structure already has indexes.\n",
    "* 'em_account_pp': All the values in the 'em_account_pp' column are 0`s. Obviously this column is irrelevant because it is not giving us any new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "em_account_pp\n",
       "0    5962924\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['em_account_pp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"Unnamed: 0\", \"em_account_pp\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Changing data types**\n",
    "\n",
    "We have two columns that inform us of dates, so the optimal thing is to convert them to datetime. But first of all, we have to change those dates that can be problematic, such as leap years. In our data we have two leap years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['entry_date'] == '2015-02-29', 'entry_date'] = '2015-02-28'\n",
    "df.loc[df['entry_date'] == '2019-02-29', 'entry_date'] = '2019-02-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pk_partition\"]=pd.to_datetime(df[\"pk_partition\"], format='%Y-%m-%d')\n",
    "df[\"entry_date\"]=pd.to_datetime(df[\"entry_date\"], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that only have a 0 or a 1 as a values, can be convert to int8 to save memory and computational costs. Because we have some nulls values we need do this step later, once we have dealt with all the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['short_term_deposit', 'loans', 'mortgage', 'funds', 'securities', 'long_term_deposit', 'credit_card',\n",
    "                      'payroll','pension_plan', 'payroll_account', 'emc_account', 'debit_card', 'em_account_p', 'active_customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIntCastingNaNError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_344/353257123.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns_to_convert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6532\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6533\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6534\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6535\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6536\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m         return self.apply(\n\u001b[0m\u001b[0;32m    415\u001b[0m             \u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m\"iu\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_astype_float_to_int_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\u001b[0m in \u001b[0;36m_astype_float_to_int_nansafe\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \"\"\"\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         raise IntCastingNaNError(\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[1;34m\"Cannot convert non-finite values (NA or inf) to integer\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         )\n",
      "\u001b[1;31mIntCastingNaNError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "for column in columns_to_convert:\n",
    "    df[column] = df[column].astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data grouping**\n",
    "\n",
    "When we have many different values for the same feature, the models may not be as efficient. It is more optimal if instead of having many values for a single feature, we have fewer. We can create new values or group those less frequent values into one.\n",
    "First of all let's see what features we can apply this procedure:\n",
    "\n",
    "* **country_id:** ES (Spain) has 5,960,672 of the total registrations, that is a 99.96% of the total. We can separate the data into \"Spain\" and \"Others.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_id\n",
       "ES    5960672\n",
       "GB        441\n",
       "FR        225\n",
       "DE        199\n",
       "US        195\n",
       "CH        194\n",
       "BR         87\n",
       "BE         81\n",
       "VE         79\n",
       "IE         68\n",
       "MX         58\n",
       "AT         51\n",
       "AR         51\n",
       "PL         49\n",
       "IT         45\n",
       "MA         34\n",
       "CL         30\n",
       "CN         28\n",
       "CA         22\n",
       "LU         17\n",
       "ET         17\n",
       "QA         17\n",
       "CI         17\n",
       "SA         17\n",
       "CM         17\n",
       "SN         17\n",
       "MR         17\n",
       "NO         17\n",
       "RU         17\n",
       "CO         17\n",
       "GA         17\n",
       "GT         17\n",
       "DO         17\n",
       "SE         16\n",
       "DJ         11\n",
       "PT         11\n",
       "JM         11\n",
       "RO          9\n",
       "HU          8\n",
       "DZ          7\n",
       "PE          4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['country_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add all the values that are not Spain in a list, and then we regroup all these values by the value \"OTHERS\". We finally see how the values of the \"country_id\" feature look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_country = df['country_id'].value_counts()[df['country_id'].value_counts() < 500 ].index.tolist()\n",
    "df.loc[df['country_id'].isin(others_country), 'country_id'] = 'OTHERS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_id\n",
       "ES        5960672\n",
       "OTHERS       2252\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['country_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **entry_channel:** To optimize the performance of the model it is advisable to regroup the variables. We will use the same criteria that we have used in Section 2 --> EDA, Analyze by visualizing data,  where we have visualized the entry_channel variable regrouping those values that have a frequency less than 2% of the total values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how we have too many values for a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE    3113947\n",
       "KFC     890620\n",
       "KHQ     590280\n",
       "KAT     416084\n",
       "KHK     230197\n",
       "KHM     176591\n",
       "KHN     108434\n",
       "KFA      79020\n",
       "KHD      75899\n",
       "RED      60601\n",
       "KHL      41736\n",
       "KHF      20657\n",
       "KHO       8247\n",
       "KAZ       5635\n",
       "KHC       5241\n",
       "KBG       1662\n",
       "KEH        728\n",
       "KHP        691\n",
       "007        613\n",
       "013        397\n",
       "KAG        335\n",
       "KAS        305\n",
       "KBZ        198\n",
       "KAA        193\n",
       "KAR        157\n",
       "KAY        149\n",
       "KAF        123\n",
       "KAB         95\n",
       "KFD         91\n",
       "KCC         89\n",
       "KAW         65\n",
       "KAQ         52\n",
       "KDH         51\n",
       "KAE         48\n",
       "KCH         48\n",
       "KCB         47\n",
       "KAD         45\n",
       "KAM         43\n",
       "KAH         40\n",
       "KAK         34\n",
       "KAJ         33\n",
       "KDT         23\n",
       "KFF         22\n",
       "KBO         21\n",
       "KEY         20\n",
       "KGC         17\n",
       "004         17\n",
       "KFS         17\n",
       "KCL         17\n",
       "KBE         17\n",
       "KBH         17\n",
       "KFK         17\n",
       "KGN         17\n",
       "KCI         17\n",
       "KGX         17\n",
       "KDR         16\n",
       "KFL         13\n",
       "KBW         12\n",
       "KBU         11\n",
       "KAI         11\n",
       "KCK         11\n",
       "KBY         11\n",
       "KES         11\n",
       "KEJ          8\n",
       "KHS          5\n",
       "KDA          2\n",
       "KFP          2\n",
       "KDS          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find values with a frequency of less than 0.02% of the total\n",
    "value_counts_entry_channel = df['entry_channel'].value_counts(normalize = True)\n",
    "others_entry_channel = value_counts_entry_channel[value_counts_entry_channel<0.02].index.tolist()\n",
    "\n",
    "# Data grouping\n",
    "df['entry_channel'] = df['entry_channel'].apply(lambda x: 'OTHERS' if x in others_entry_channel else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE       3113947\n",
       "KFC        890620\n",
       "KHQ        590280\n",
       "KAT        416084\n",
       "OTHERS     412172\n",
       "KHK        230197\n",
       "KHM        176591\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Handling missing values**\n",
    "\n",
    "We need to fill in the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of nulls of payroll ---> 61\n",
      "Total sum of nulls of pension_plan ---> 61\n",
      "Total sum of nulls of entry_channel ---> 133033\n",
      "Total sum of nulls of segment ---> 133944\n",
      "Total sum of nulls of region_code ---> 2264\n",
      "Total sum of nulls of gender ---> 25\n",
      "Total sum of nulls of salary ---> 1512103\n"
     ]
    }
   ],
   "source": [
    "# number of nulls\n",
    "for i in df.columns.values:\n",
    "    if df[i].isnull().sum()>0:\n",
    "        print(f\"Total sum of nulls of {i} --->\",df[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **salary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.358414764300196"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['salary'].isnull().sum()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **entry_channel:** For Imputing the null values present in the categorical columns we used mode impuation. However \"entry_channel\" has 2.23% of missing values, which is large enough to change the distribution of the data, which we don't want. So we will impute the values according to the class weights to keep the data distribution uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2310027764901914"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df['entry_channel'].isnull().sum()/df.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE       3113947\n",
       "KFC        890620\n",
       "KHQ        590280\n",
       "KAT        416084\n",
       "OTHERS     412172\n",
       "KHK        230197\n",
       "KHM        176591\n",
       "NaN        133033\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry_channel'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **segment:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **region_code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **gender:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **payroll , pension_plan:****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating new variables:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Encoding categorical variables:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Studying correlations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Studying variance:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
