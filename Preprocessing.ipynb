{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Wrangle, prepare, cleanse the data**\n",
    "\n",
    "In this section, we will prepare the data to ensure that the model operates optimally. We will follow a set of steps based on the analysis we conducted in Section 2 Data Analysis. The phases we will work on include:\n",
    "\n",
    "**3.1 Cleaning:** Removing duplicates, correcting incorrect column names and removing irrelevant columns.\\\n",
    "**3.2 Integration:** Data grouping, handling missing values and changing data types\\\n",
    "**3.3 Construction:** Creating new features and encoding categorical features.\\\n",
    "**3.4 Feature selection:** Studying correlations and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,OrdinalEncoder,StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquire data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_easymoney.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Removing duplicates**\n",
    "\n",
    "We note that we do not have any duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Correcting incorrect column names**\n",
    "\n",
    "We can see how the em_acount variable is misspelled, it should be em_account. Not correcting this error, no matter how insignificant it may seem now, can lead to problems later when working with this column. We proceed to correct the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'pk_cid', 'pk_partition', 'short_term_deposit', 'loans',\n",
       "       'mortgage', 'funds', 'securities', 'long_term_deposit', 'em_account_pp',\n",
       "       'credit_card', 'payroll', 'pension_plan', 'payroll_account',\n",
       "       'emc_account', 'debit_card', 'em_account_p', 'em_acount', 'entry_date',\n",
       "       'entry_channel', 'active_customer', 'segment', 'country_id',\n",
       "       'region_code', 'gender', 'age', 'deceased', 'salary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {\"em_acount\":\"em_account\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Removing inrrelevant columns**\n",
    "\n",
    "Although later we will do correlation and variance studies, where we will delve into those columns that may be irrelevant, taking a quick look we can see that:\n",
    "\n",
    "* 'Unnamed:0' : This column acts as an inndex of the rows of our sample. This column is obviusly irrelevant since the dataframe structure already has indexes.\n",
    "* 'em_account_pp': All the values in the 'em_account_pp' column are 0`s. Obviously this column is irrelevant because it is not giving us any new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "em_account_pp\n",
       "0    5962924\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['em_account_pp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"Unnamed: 0\", \"em_account_pp\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data grouping**\n",
    "\n",
    "When we have many different values for the same feature, the models may not be as efficient. It is more optimal if instead of having many values for a single feature, we have fewer. We can create new values or group those less frequent values into one.\n",
    "First of all let's see what features we can apply this procedure:\n",
    "\n",
    "* **country_id:** ES (Spain) has 5,960,672 of the total registrations, that is a 99.96% of the total. We can separate the data into \"Spain\" and \"Others.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_id\n",
       "ES    5960672\n",
       "GB        441\n",
       "FR        225\n",
       "DE        199\n",
       "US        195\n",
       "CH        194\n",
       "BR         87\n",
       "BE         81\n",
       "VE         79\n",
       "IE         68\n",
       "MX         58\n",
       "AT         51\n",
       "AR         51\n",
       "PL         49\n",
       "IT         45\n",
       "MA         34\n",
       "CL         30\n",
       "CN         28\n",
       "CA         22\n",
       "LU         17\n",
       "ET         17\n",
       "QA         17\n",
       "CI         17\n",
       "SA         17\n",
       "CM         17\n",
       "SN         17\n",
       "MR         17\n",
       "NO         17\n",
       "RU         17\n",
       "CO         17\n",
       "GA         17\n",
       "GT         17\n",
       "DO         17\n",
       "SE         16\n",
       "DJ         11\n",
       "PT         11\n",
       "JM         11\n",
       "RO          9\n",
       "HU          8\n",
       "DZ          7\n",
       "PE          4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['country_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add all the values that are not Spain in a list, and then we regroup all these values by the value \"OTHERS\". We finally see how the values of the \"country_id\" feature look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_country = df['country_id'].value_counts()[df['country_id'].value_counts() < 500 ].index.tolist()\n",
    "df.loc[df['country_id'].isin(others_country), 'country_id'] = 'OTHERS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_id\n",
       "ES        5960672\n",
       "OTHERS       2252\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['country_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **entry_channel:** To optimize the performance of the model it is advisable to regroup the variables. We will use the same criteria that we have used in Section 2 --> EDA, Analyze by visualizing data,  where we have visualized the entry_channel variable regrouping those values that have a frequency less than 2% of the total values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how we have too many values for a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE    3113947\n",
       "KFC     890620\n",
       "KHQ     590280\n",
       "KAT     416084\n",
       "KHK     230197\n",
       "        ...   \n",
       "KEJ          8\n",
       "KHS          5\n",
       "KDA          2\n",
       "KFP          2\n",
       "KDS          1\n",
       "Name: count, Length: 68, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find values with a frequency of less than 0.02% of the total\n",
    "value_counts_entry_channel = df['entry_channel'].value_counts(normalize = True)\n",
    "others_entry_channel = value_counts_entry_channel[value_counts_entry_channel<0.02].index.tolist()\n",
    "\n",
    "# Data grouping\n",
    "df['entry_channel'] = df['entry_channel'].apply(lambda x: 'OTHERS' if x in others_entry_channel else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE       3113947\n",
       "KFC        890620\n",
       "KHQ        590280\n",
       "KAT        416084\n",
       "OTHERS     412172\n",
       "KHK        230197\n",
       "KHM        176591\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Handling missing values**\n",
    "\n",
    "We need to fill in the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of nulls of payroll ---> 61\n",
      "Total sum of nulls of pension_plan ---> 61\n",
      "Total sum of nulls of entry_channel ---> 133033\n",
      "Total sum of nulls of segment ---> 133944\n",
      "Total sum of nulls of region_code ---> 2264\n",
      "Total sum of nulls of gender ---> 25\n",
      "Total sum of nulls of salary ---> 1512103\n"
     ]
    }
   ],
   "source": [
    "# number of nulls\n",
    "for i in df.columns.values:\n",
    "    if df[i].isnull().sum()>0:\n",
    "        print(f\"Total sum of nulls of {i} --->\",df[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **salary:** We have 25% nulls in our dataset. It is a large enough amount to be able to eliminate the variable, however, the salary variable can be important when creating clustering and churn models. Therefore, when imputing nulls we must be very careful with the values we enter.  \n",
    "Socially, the two factors that have the most relevance in the **salary** feature are **age and gender**. Therefore, those records that contain nulls in the 'salary' feature will be imputed with the average of 'salary' of the records that have the same characteristics of the null's records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.358414764300196"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of nulls\n",
    "(df['salary'].isnull().sum()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_imputation(df):\n",
    "    \n",
    "    #Age range\n",
    "    ages = [0,16,21,26,31,36,41,46,51,56,61,66]\n",
    "    #We will add average salaries to these lists based on age and gender\n",
    "    average_salary_V = []\n",
    "    average_salary_H = []\n",
    "        \n",
    "    for i in range(len(ages)): \n",
    "        if i<(len(ages)-1):\n",
    "            #We add the means based on the characteristics of the sample\n",
    "            average_salary_V.append(df.loc[(df['gender'] == 'V') & (df['age'] >= ages[i]) & (df['age'] < ages[i+1]) & (df['salary'].notnull()), 'salary'].mean())\n",
    "            average_salary_H.append(df.loc[(df['gender'] == 'H') & (df['age'] >= ages[i]) & (df['age'] < ages[i+1]) & (df['salary'].notnull()), 'salary'].mean())\n",
    "            \n",
    "            #We impute nulls based on which group the nulls belong to\n",
    "            df.loc[(df['gender'] == 'V') & (df['age'] >= ages[i]) & (df['age'] < ages[i+1]) & (df['salary'].isnull()), 'salary'] = average_salary_V[i]\n",
    "            df.loc[(df['gender'] == 'H') & (df['age'] >= ages[i]) & (df['age'] < ages[i+1]) & (df['salary'].isnull()), 'salary'] = average_salary_H[i]\n",
    "        \n",
    "        else:\n",
    "            #Same case for population of 66+\n",
    "            average_salary_V.append(df.loc[(df['gender'] == 'V')  & (df['age'] >= ages[i]) & (df['salary'].notnull()), 'salary'].mean())\n",
    "            average_salary_H.append(df.loc[(df['gender'] == 'H')  & (df['age'] >= ages[i]) & (df['salary'].notnull()), 'salary'].mean())\n",
    "            \n",
    "            df.loc[(df['gender'] == 'V')  & (df['age'] >= ages[i]) & (df['salary'].isnull()), 'salary'] = average_salary_V[i]\n",
    "            df.loc[(df['gender'] == 'H')  & (df['age'] >= ages[i]) & (df['salary'].isnull()), 'salary'] = average_salary_H[i]\n",
    "            \n",
    "            \n",
    "    return print(f\"Total sum of nulls of 'salary' --->\", df['salary'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of nulls of 'salary' ---> 0\n"
     ]
    }
   ],
   "source": [
    "salary_imputation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **entry_channel:** We have a fairly representative sample, therefore we will use the mode to impute null values. However \"entry_channel\" has 2.23% of missing values, which is large enough to change the distribution of the data, which we don't want. So we will impute nulls based on relative frequency of the feature to keep the initial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2310027764901914"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % missing values from dataset\n",
    "(df['entry_channel'].isnull().sum()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we calculate the relative frequency of the variable 'entry_channel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE       0.534135\n",
       "KFC       0.152768\n",
       "KHQ       0.101251\n",
       "KAT       0.071371\n",
       "OTHERS    0.070700\n",
       "KHK       0.039486\n",
       "KHM       0.030291\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_frequency_entry_channel = df['entry_channel'].value_counts(normalize=True)\n",
    "relative_frequency_entry_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impute to the 'entry_channel_nulls' random values, always belonging to the 'entry_channel' feature. This way the relative frequency remains as at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_channel_nulls = df['entry_channel'].isnull()\n",
    "df.loc[entry_channel_nulls, 'entry_channel'] = np.random.choice(relative_frequency_entry_channel.index, size=entry_channel_nulls.sum(), p=relative_frequency_entry_channel.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **segment:** 'segment' feature has 2.24% null values. We will proceed with the same procedure that we have followed for the 'entry_channel' feature. So we will impute the nulls based on keeping the final distribution as the intial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.246280516068962"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % missing values from dataset\n",
    "(df['segment'].isnull().sum()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we calculate the relative frequency of the variable 'segment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "segment\n",
       "03 - UNIVERSITARIO    0.669099\n",
       "02 - PARTICULARES     0.314099\n",
       "01 - TOP              0.016802\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_frequency_segment = df['segment'].value_counts(normalize=True)\n",
    "relative_frequency_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impute to the 'segment_nulls' random values, always belonging to the 'segment' feature. This way the relative frequency remains as at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_nulls = df['segment'].isnull()\n",
    "df.loc[segment_nulls, 'segment'] = np.random.choice(relative_frequency_segment.index, size=segment_nulls.sum(), p=relative_frequency_segment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **region_code:** We impute the mode. Only the 0.03% are null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region_code'].fillna(df['region_code'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **gender:**  We impute the mode. Only the 0.0004% are null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'].fillna(df['gender'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **payroll , pension_plan:** These two features explian if the client has contracted the product. We understand a null value as a client who has not contracted the product and for various reasons the value 0 hasn't been entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['payroll'].fillna(0, inplace = True)\n",
    "df['pension_plan'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure that every step has been completed right and we have a dataframe clean of nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pk_cid                0\n",
       "pk_partition          0\n",
       "short_term_deposit    0\n",
       "loans                 0\n",
       "mortgage              0\n",
       "funds                 0\n",
       "securities            0\n",
       "long_term_deposit     0\n",
       "credit_card           0\n",
       "payroll               0\n",
       "pension_plan          0\n",
       "payroll_account       0\n",
       "emc_account           0\n",
       "debit_card            0\n",
       "em_account_p          0\n",
       "em_account            0\n",
       "entry_date            0\n",
       "entry_channel         0\n",
       "active_customer       0\n",
       "segment               0\n",
       "country_id            0\n",
       "region_code           0\n",
       "gender                0\n",
       "age                   0\n",
       "deceased              0\n",
       "salary                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Changing data types**\n",
    "\n",
    "We have two columns that inform us of dates, so the optimal thing is to convert them to datetime. But first of all, we have to change those dates that can be problematic, such as leap years. In our data we have two leap years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['entry_date'] == '2015-02-29', 'entry_date'] = '2015-02-28'\n",
    "df.loc[df['entry_date'] == '2019-02-29', 'entry_date'] = '2019-02-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pk_partition\"]=pd.to_datetime(df[\"pk_partition\"], format='%Y-%m-%d')\n",
    "df[\"entry_date\"]=pd.to_datetime(df[\"entry_date\"], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that only have a 0 or a 1 as a values, can be convert to int8 to save memory and computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['short_term_deposit', 'loans', 'mortgage', 'funds', 'securities', 'long_term_deposit', 'credit_card',\n",
    "                      'payroll','pension_plan', 'payroll_account', 'emc_account', 'debit_card', 'em_account_p', 'active_customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_convert:\n",
    "    df[column] = df[column].astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3 Construccion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Creating new features:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating news features from existing data is important for several reasons in order to optimize model performance:  \n",
    "   \n",
    "**Improve model accuracy:** New features can contain valuable information that was not explicitly represented in the original features.  \n",
    "**Dimensionality reduction:** New features can be created to reduce the dimensionality of the dataset while retaining essential information.  \n",
    "**Model interpretability:** Introducing news features may lead to more interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have just explained, the creation of new features is done for a very specific purpose. Therefore, in each part of the project, the features that are required will be created.\n",
    "\n",
    "For now, in the preprocessing notebook we will only create the first group of variables for the general df. However, we already proposed which variables we are going to create later in the following steps\n",
    "\n",
    "**Creating new features for the general df:**\n",
    "* Total products per client and per pk_partition\n",
    "* Total products account per client and per pk_partition\n",
    "* Total products financing per client and per pk_partition\n",
    "* Total products saving/investments per client and per pk_partition\n",
    "\n",
    "\n",
    "**Creating new features for the df segmentation:**\n",
    "* Total current products \n",
    "* Total current products account \n",
    "* Total current products financing  \n",
    "* Total current products savings/investments  \n",
    "* Registration time\n",
    "* Total revenues\n",
    "* Total registrations\n",
    "* Total cancellations\n",
    "* Average duration of products\n",
    "\n",
    "\n",
    "**Creating new features for the df churn model:**\n",
    "* Products account + shift(1)\n",
    "* Products financing + shift(1)\n",
    "* Products saving/investments + shitf(1)\n",
    "* Products account + shift(2)\n",
    "* Products financing + shift(2)\n",
    "* Products saving/investments + shitf(2)\n",
    "* Products account + shift(3)\n",
    "* Products financing + shift(3)\n",
    "* Products saving/investments + shitf(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Creating new features for the general df:** First of all we need to create the following lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_columns = ['short_term_deposit', 'loans', 'mortgage',\n",
    "       'funds', 'securities', 'long_term_deposit', 'credit_card', 'payroll',\n",
    "       'pension_plan', 'payroll_account', 'emc_account', 'debit_card',\n",
    "       'em_account_p', 'em_account']\n",
    "\n",
    "account_prod = ['payroll', 'payroll_account', 'emc_account', 'em_account_p', 'em_account', \n",
    "                'debit_card']\n",
    "financing_prod = ['loans', 'mortgage', 'credit_card']\n",
    "saving_prod = ['short_term_deposit', 'funds', 'securities', 'long_term_deposit',\n",
    "       'pension_plan' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_products'] = df[prod_columns].sum(axis=1)\n",
    "\n",
    "df['account_prod'] = df[account_prod].sum(axis=1)\n",
    "df['financing_prod'] = df[financing_prod].sum(axis=1)\n",
    "df['saving_prod'] = df[saving_prod].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_products</th>\n",
       "      <th>account_prod</th>\n",
       "      <th>financing_prod</th>\n",
       "      <th>saving_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4555563</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907417</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894938</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976687</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851229</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927073</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286457</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561681</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171647</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         total_products  account_prod  financing_prod  saving_prod\n",
       "4555563               1             1               0            0\n",
       "1650141               0             0               0            0\n",
       "5907417               1             1               0            0\n",
       "1894938               0             0               0            0\n",
       "3976687               1             1               0            0\n",
       "5851229               0             0               0            0\n",
       "5927073               0             0               0            0\n",
       "286457                1             1               0            0\n",
       "3561681               0             0               0            0\n",
       "171647                1             1               0            0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df sample only with the new features\n",
    "df[['total_products', 'account_prod', 'financing_prod', 'saving_prod']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Encoding categorical variables:**\n",
    "\n",
    "To be able to make a model that performs well, we need to have all the variables be numerical. We will use different encoding strategies depending on the variables to be treated:\n",
    "\n",
    "* **Boolean encoding:** country_id, gender, deceased\n",
    "* **LabelEncoder:** entry_channel (This approach is appropriate when there is no inherent order to the categories and you want to assign a numerical value to each category uniquely.)\n",
    "* **OrdinalEncoder:** segment (Assign whole numbers to each category based on their order or importance. This strategy is used when there is an inherent order in the categories.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boolean encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country_id'] = df['country_id'].map({'OTHERS' : 0, 'ES': 1})\n",
    "df['gender'] = df['gender'].map({'H' : 0, 'V' : 1})\n",
    "df['deceased'] = df['deceased'].map({'N' : 0, 'S' : 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LabelEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['entry_channel'] = label_encoder.fit_transform(df['entry_channel']).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OrdinalEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'03 - UNIVERSITARIO':1, '02 - PARTICULARES':2, '01 - TOP':3}\n",
    "df['segment'] = df['segment'].map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how we have the preprocessing of our dataset almost ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pk_cid</th>\n",
       "      <td>1375586</td>\n",
       "      <td>1050611</td>\n",
       "      <td>1050612</td>\n",
       "      <td>1050613</td>\n",
       "      <td>1050614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pk_partition</th>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_term_deposit</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loans</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mortgage</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funds</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>securities</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_deposit</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit_card</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payroll</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pension_plan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payroll_account</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emc_account</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debit_card</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>em_account_p</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>em_account</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry_date</th>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>2015-08-10 00:00:00</td>\n",
       "      <td>2015-08-10 00:00:00</td>\n",
       "      <td>2015-08-10 00:00:00</td>\n",
       "      <td>2015-08-10 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry_channel</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_customer</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country_id</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_code</th>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>35</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deceased</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>87218.1</td>\n",
       "      <td>35548.74</td>\n",
       "      <td>122179.11</td>\n",
       "      <td>119775.54</td>\n",
       "      <td>119974.545133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_products</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account_prod</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financing_prod</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saving_prod</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0                    1  \\\n",
       "pk_cid                          1375586              1050611   \n",
       "pk_partition        2018-01-28 00:00:00  2018-01-28 00:00:00   \n",
       "short_term_deposit                    0                    0   \n",
       "loans                                 0                    0   \n",
       "mortgage                              0                    0   \n",
       "funds                                 0                    0   \n",
       "securities                            0                    0   \n",
       "long_term_deposit                     0                    0   \n",
       "credit_card                           0                    0   \n",
       "payroll                               0                    0   \n",
       "pension_plan                          0                    0   \n",
       "payroll_account                       0                    0   \n",
       "emc_account                           0                    0   \n",
       "debit_card                            0                    0   \n",
       "em_account_p                          0                    0   \n",
       "em_account                            1                    1   \n",
       "entry_date          2018-01-12 00:00:00  2015-08-10 00:00:00   \n",
       "entry_channel                         6                    2   \n",
       "active_customer                       1                    0   \n",
       "segment                               2                    1   \n",
       "country_id                            1                    1   \n",
       "region_code                        29.0                 13.0   \n",
       "gender                                0                    1   \n",
       "age                                  35                   23   \n",
       "deceased                              0                    0   \n",
       "salary                          87218.1             35548.74   \n",
       "total_products                        1                    1   \n",
       "account_prod                          1                    1   \n",
       "financing_prod                        0                    0   \n",
       "saving_prod                           0                    0   \n",
       "\n",
       "                                      2                    3  \\\n",
       "pk_cid                          1050612              1050613   \n",
       "pk_partition        2018-01-28 00:00:00  2018-01-28 00:00:00   \n",
       "short_term_deposit                    0                    1   \n",
       "loans                                 0                    0   \n",
       "mortgage                              0                    0   \n",
       "funds                                 0                    0   \n",
       "securities                            0                    0   \n",
       "long_term_deposit                     0                    0   \n",
       "credit_card                           0                    0   \n",
       "payroll                               0                    0   \n",
       "pension_plan                          0                    0   \n",
       "payroll_account                       0                    0   \n",
       "emc_account                           0                    0   \n",
       "debit_card                            0                    0   \n",
       "em_account_p                          0                    0   \n",
       "em_account                            1                    0   \n",
       "entry_date          2015-08-10 00:00:00  2015-08-10 00:00:00   \n",
       "entry_channel                         2                    6   \n",
       "active_customer                       0                    0   \n",
       "segment                               1                    1   \n",
       "country_id                            1                    1   \n",
       "region_code                        13.0                 50.0   \n",
       "gender                                1                    0   \n",
       "age                                  23                   22   \n",
       "deceased                              0                    0   \n",
       "salary                        122179.11            119775.54   \n",
       "total_products                        1                    1   \n",
       "account_prod                          1                    0   \n",
       "financing_prod                        0                    0   \n",
       "saving_prod                           0                    1   \n",
       "\n",
       "                                      4  \n",
       "pk_cid                          1050614  \n",
       "pk_partition        2018-01-28 00:00:00  \n",
       "short_term_deposit                    0  \n",
       "loans                                 0  \n",
       "mortgage                              0  \n",
       "funds                                 0  \n",
       "securities                            0  \n",
       "long_term_deposit                     0  \n",
       "credit_card                           0  \n",
       "payroll                               0  \n",
       "pension_plan                          0  \n",
       "payroll_account                       0  \n",
       "emc_account                           0  \n",
       "debit_card                            0  \n",
       "em_account_p                          0  \n",
       "em_account                            1  \n",
       "entry_date          2015-08-10 00:00:00  \n",
       "entry_channel                         2  \n",
       "active_customer                       1  \n",
       "segment                               1  \n",
       "country_id                            1  \n",
       "region_code                        50.0  \n",
       "gender                                1  \n",
       "age                                  23  \n",
       "deceased                              0  \n",
       "salary                    119974.545133  \n",
       "total_products                        1  \n",
       "account_prod                          1  \n",
       "financing_prod                        0  \n",
       "saving_prod                           0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.4 Feature selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Studying correlations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Pandas requires version '3.1.2' or newer of 'jinja2' (version '3.0.2' currently installed).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4436/1933976207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcorrelation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorrelation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackground_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"coolwarm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mstyle\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1339\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mTable\u001b[0m \u001b[0mVisualization\u001b[0m \u001b[1;33m<\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0muser_guide\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipynb\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \"\"\"\n\u001b[1;32m-> 1341\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStyler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mStyler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\style.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_to_buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mjinja2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jinja2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"DataFrame.style requires jinja2.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m from pandas.io.formats.style_render import (\n",
      "\u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Pandas requires version '3.1.2' or newer of 'jinja2' (version '3.0.2' currently installed)."
     ]
    }
   ],
   "source": [
    "correlation = df.corr();\n",
    "correlation.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Studying variance:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
