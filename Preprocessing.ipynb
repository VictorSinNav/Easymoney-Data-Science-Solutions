{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Wrangle, prepare, cleanse the data**\n",
    "\n",
    "In this section, we will prepare the data to ensure that the model operates optimally. We will follow a set of steps based on the analysis we conducted in Section 2 Data Analysis. The phases we will work on include:\n",
    "\n",
    "**3.1 Cleaning:** Removing duplicates, correcting incorrect column names and removing irrelevant columns.\\\n",
    "**3.2 Integration:** Data grouping, handling missing values and changing data types\\\n",
    "**3.3 Construction:** Creating new variables and encoding categorical variables.\\\n",
    "**3.4 Variable selection:** Studying correlations and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,OrdinalEncoder,StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquire data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_easymoney.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Removing duplicates**\n",
    "\n",
    "We note that we do not have any duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Correcting incorrect column names**\n",
    "\n",
    "We can see how the em_acount variable is misspelled, it should be em_account. Not correcting this error, no matter how insignificant it may seem now, can lead to problems later when working with this column. We proceed to correct the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'pk_cid', 'pk_partition', 'short_term_deposit', 'loans',\n",
       "       'mortgage', 'funds', 'securities', 'long_term_deposit', 'em_account_pp',\n",
       "       'credit_card', 'payroll', 'pension_plan', 'payroll_account',\n",
       "       'emc_account', 'debit_card', 'em_account_p', 'em_acount', 'entry_date',\n",
       "       'entry_channel', 'active_customer', 'segment', 'country_id',\n",
       "       'region_code', 'gender', 'age', 'deceased', 'salary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {\"em_acount\":\"em_account\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Removing inrrelevant columns**\n",
    "\n",
    "Although later we will do correlation and variance studies, where we will delve into those columns that may be irrelevant, taking a quick look we can see that:\n",
    "\n",
    "* 'Unnamed:0' : This column acts as an inndex of the rows of our sample. This column is obviusly irrelevant since the dataframe structure already has indexes.\n",
    "* 'em_account_pp': All the values in the 'em_account_pp' column are 0`s. Obviously this column is irrelevant because it is not giving us any new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "em_account_pp\n",
       "0    5962924\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['em_account_pp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"Unnamed: 0\", \"em_account_pp\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data grouping**\n",
    "\n",
    "When we have many different values for the same feature, the models may not be as efficient. It is more optimal if instead of having many values for a single feature, we have fewer. We can create new values or group those less frequent values into one.\n",
    "First of all let's see what features we can apply this procedure:\n",
    "\n",
    "* **country_id:** ES (Spain) has 5,960,672 of the total registrations, that is a 99.96% of the total. We can separate the data into \"Spain\" and \"Others.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_id\n",
       "ES    5960672\n",
       "GB        441\n",
       "FR        225\n",
       "DE        199\n",
       "US        195\n",
       "CH        194\n",
       "BR         87\n",
       "BE         81\n",
       "VE         79\n",
       "IE         68\n",
       "MX         58\n",
       "AT         51\n",
       "AR         51\n",
       "PL         49\n",
       "IT         45\n",
       "MA         34\n",
       "CL         30\n",
       "CN         28\n",
       "CA         22\n",
       "LU         17\n",
       "ET         17\n",
       "QA         17\n",
       "CI         17\n",
       "SA         17\n",
       "CM         17\n",
       "SN         17\n",
       "MR         17\n",
       "NO         17\n",
       "RU         17\n",
       "CO         17\n",
       "GA         17\n",
       "GT         17\n",
       "DO         17\n",
       "SE         16\n",
       "DJ         11\n",
       "PT         11\n",
       "JM         11\n",
       "RO          9\n",
       "HU          8\n",
       "DZ          7\n",
       "PE          4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['country_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add all the values that are not Spain in a list, and then we regroup all these values by the value \"OTHERS\". We finally see how the values of the \"country_id\" feature look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_country = df['country_id'].value_counts()[df['country_id'].value_counts() < 500 ].index.tolist()\n",
    "df.loc[df['country_id'].isin(others_country), 'country_id'] = 'OTHERS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_id\n",
       "ES        5960672\n",
       "OTHERS       2252\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['country_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **entry_channel:** To optimize the performance of the model it is advisable to regroup the variables. We will use the same criteria that we have used in Section 2 --> EDA, Analyze by visualizing data,  where we have visualized the entry_channel variable regrouping those values that have a frequency less than 2% of the total values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how we have too many values for a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE    3113947\n",
       "KFC     890620\n",
       "KHQ     590280\n",
       "KAT     416084\n",
       "KHK     230197\n",
       "        ...   \n",
       "KEJ          8\n",
       "KHS          5\n",
       "KDA          2\n",
       "KFP          2\n",
       "KDS          1\n",
       "Name: count, Length: 68, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find values with a frequency of less than 0.02% of the total\n",
    "value_counts_entry_channel = df['entry_channel'].value_counts(normalize = True)\n",
    "others_entry_channel = value_counts_entry_channel[value_counts_entry_channel<0.02].index.tolist()\n",
    "\n",
    "# Data grouping\n",
    "df['entry_channel'] = df['entry_channel'].apply(lambda x: 'OTHERS' if x in others_entry_channel else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE       3113947\n",
       "KFC        890620\n",
       "KHQ        590280\n",
       "KAT        416084\n",
       "OTHERS     412172\n",
       "KHK        230197\n",
       "KHM        176591\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Handling missing values**\n",
    "\n",
    "We need to fill in the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of nulls of payroll ---> 61\n",
      "Total sum of nulls of pension_plan ---> 61\n",
      "Total sum of nulls of entry_channel ---> 133033\n",
      "Total sum of nulls of segment ---> 133944\n",
      "Total sum of nulls of region_code ---> 2264\n",
      "Total sum of nulls of gender ---> 25\n",
      "Total sum of nulls of salary ---> 1512103\n"
     ]
    }
   ],
   "source": [
    "# number of nulls\n",
    "for i in df.columns.values:\n",
    "    if df[i].isnull().sum()>0:\n",
    "        print(f\"Total sum of nulls of {i} --->\",df[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **salary:** We have 25% nulls in our dataset. It is a large enough amount to be able to eliminate the variable, however, the salary variable can be important when creating clustering and churn models. Therefore, when imputing nulls we must be very careful with the values we enter.\\\n",
    "Socially, the two factors that have the most relevance in the **salary** feature are **age and gender**. Therefore, those records that contain nulls in the 'salary' feature will be imputed with the average of 'salary' of the records that have the same characteristics of the null's records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.358414764300196"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of nulls\n",
    "(df['salary'].isnull().sum()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_imputation(df):\n",
    "    \n",
    "    #Age range\n",
    "    ages = [0,16,21,26,31,36,41,46,51,56,61,66]\n",
    "    #We will add average salaries to these lists based on age and gender\n",
    "    average_salary_V = []\n",
    "    average_salary_H = []\n",
    "        \n",
    "    for i in range(len(ages)): \n",
    "        if i<(len(ages)-1):\n",
    "            #We add the means based on the characteristics of the sample\n",
    "            average_salary_V.append(df.loc[(df['gender'] == 'V') & (df['age'] >= ages[i]) & (df['age'] < ages[i+1]) & (df['salary'].notnull()), 'salary'].mean())\n",
    "            average_salary_H.append(df.loc[(df['gender'] == 'H') & (df['age'] >= ages[i]) & (df['age'] < ages[i+1]) & (df['salary'].notnull()), 'salary'].mean())\n",
    "            \n",
    "            #We impute nulls based on which group the nulls belong to\n",
    "            df.loc[(df['gender'] == 'V') & (df['age'] >= ages[i]) & (df['age'] < ages[i+1]) & (df['salary'].isnull()), 'salary'] = average_salary_V[i]\n",
    "            df.loc[(df['gender'] == 'H') & (df['age'] >= ages[i]) & (df['age'] < ages[i+1]) & (df['salary'].isnull()), 'salary'] = average_salary_H[i]\n",
    "        \n",
    "        else:\n",
    "            #Same case for population of 60+\n",
    "            average_salary_V.append(df.loc[(df['gender'] == 'V')  & (df['age'] >= ages[i]) & (df['salary'].notnull()), 'salary'].mean())\n",
    "            average_salary_H.append(df.loc[(df['gender'] == 'H')  & (df['age'] >= ages[i]) & (df['salary'].notnull()), 'salary'].mean())\n",
    "            \n",
    "            df.loc[(df['gender'] == 'V')  & (df['age'] >= ages[i]) & (df['salary'].isnull()), 'salary'] = average_salary_V[i]\n",
    "            df.loc[(df['gender'] == 'H')  & (df['age'] >= ages[i]) & (df['salary'].isnull()), 'salary'] = average_salary_H[i]\n",
    "            \n",
    "            \n",
    "    return print(f\"Total sum of nulls of 'salary' --->\", df['salary'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of nulls of 'salary' ---> 0\n"
     ]
    }
   ],
   "source": [
    "salary_imputation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **entry_channel:** We have a fairly representative sample, therefore we will use the mode to impute null values. However \"entry_channel\" has 2.23% of missing values, which is large enough to change the distribution of the data, which we don't want. So we will impute nulls based on relative frequency of the feature to keep the initial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2310027764901914"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % missing values from dataset\n",
    "(df['entry_channel'].isnull().sum()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we calculate the relative frequency of the variable 'entry_channel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entry_channel\n",
       "KHE       0.534135\n",
       "KFC       0.152768\n",
       "KHQ       0.101251\n",
       "KAT       0.071371\n",
       "OTHERS    0.070700\n",
       "KHK       0.039486\n",
       "KHM       0.030291\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_frequency_entry_channel = df['entry_channel'].value_counts(normalize=True)\n",
    "relative_frequency_entry_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impute to the 'entry_channel_nulls' random values, always belonging to the 'entry_channel' feature. This way the relative frequency remains as at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_channel_nulls = df['entry_channel'].isnull()\n",
    "df.loc[entry_channel_nulls, 'entry_channel'] = np.random.choice(relative_frequency_entry_channel.index, size=entry_channel_nulls.sum(), p=relative_frequency_entry_channel.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **segment:** 'segment' feature has 2.24% null values. We will proceed with the same procedure that we have followed for the 'entry_channel' feature. So we will impute the nulls based on keeping the final distribution as the intial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.246280516068962"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % missing values from dataset\n",
    "(df['segment'].isnull().sum()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we calculate the relative frequency of the variable 'segment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "segment\n",
       "03 - UNIVERSITARIO    0.669099\n",
       "02 - PARTICULARES     0.314099\n",
       "01 - TOP              0.016802\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_frequency_segment = df['segment'].value_counts(normalize=True)\n",
    "relative_frequency_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impute to the 'segment_nulls' random values, always belonging to the 'segment' feature. This way the relative frequency remains as at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_nulls = df['segment'].isnull()\n",
    "df.loc[segment_nulls, 'segment'] = np.random.choice(relative_frequency_segment.index, size=segment_nulls.sum(), p=relative_frequency_segment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **region_code:** We impute the mode. Only the 0.03% are null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region_code'].fillna(df['region_code'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **gender:**  We impute the mode. Only the 0.0004% are null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'].fillna(df['gender'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **payroll , pension_plan:** These two features explian if the client has contracted the product. We understand a null value as a client who has not contracted the product and for various reasons the value 0 hasn't been entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['payroll'].fillna(0, inplace = True)\n",
    "df['pension_plan'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure that every step has been completed right and we have a dataframe clean of nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pk_cid                0\n",
       "pk_partition          0\n",
       "short_term_deposit    0\n",
       "loans                 0\n",
       "mortgage              0\n",
       "funds                 0\n",
       "securities            0\n",
       "long_term_deposit     0\n",
       "credit_card           0\n",
       "payroll               0\n",
       "pension_plan          0\n",
       "payroll_account       0\n",
       "emc_account           0\n",
       "debit_card            0\n",
       "em_account_p          0\n",
       "em_account            0\n",
       "entry_date            0\n",
       "entry_channel         0\n",
       "active_customer       0\n",
       "segment               0\n",
       "country_id            0\n",
       "region_code           0\n",
       "gender                0\n",
       "age                   0\n",
       "deceased              0\n",
       "salary                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Changing data types**\n",
    "\n",
    "We have two columns that inform us of dates, so the optimal thing is to convert them to datetime. But first of all, we have to change those dates that can be problematic, such as leap years. In our data we have two leap years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['entry_date'] == '2015-02-29', 'entry_date'] = '2015-02-28'\n",
    "df.loc[df['entry_date'] == '2019-02-29', 'entry_date'] = '2019-02-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pk_partition\"]=pd.to_datetime(df[\"pk_partition\"], format='%Y-%m-%d')\n",
    "df[\"entry_date\"]=pd.to_datetime(df[\"entry_date\"], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that only have a 0 or a 1 as a values, can be convert to int8 to save memory and computational costs. Because we have some nulls values we need do this step later, once we have dealt with all the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['short_term_deposit', 'loans', 'mortgage', 'funds', 'securities', 'long_term_deposit', 'credit_card',\n",
    "                      'payroll','pension_plan', 'payroll_account', 'emc_account', 'debit_card', 'em_account_p', 'active_customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_convert:\n",
    "    df[column] = df[column].astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating new variables:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Encoding categorical variables:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Studying correlations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Studying variance:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
